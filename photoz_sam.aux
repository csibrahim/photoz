\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{rasmussen2006gaussian}
\citation{zhang2005time}
\citation{tsiligkaridis2013}
\citation{gibbs97}
\citation{foster2009}
\citation{snelson2005}
\citation{laureijs2011}
\citation{laureijs2011}
\newlabel{firstpage}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{rasmussen2006gaussian}
\citation{rasmussen2006gaussian}
\citation{rasmussen2006gaussian}
\citation{roberts2012rs}
\@writefile{toc}{\contentsline {section}{\numberline {2}Gaussian Processes}{2}{section.2}}
\newlabel{sec-gaussian-process}{{2}{2}{Gaussian Processes}{section.2}{}}
\newlabel{eq-conditional-zero-mean}{{3}{2}{Gaussian Processes}{equation.2.3}{}}
\newlabel{eq-mean-variance-noise}{{4}{2}{Gaussian Processes}{equation.2.4}{}}
\newlabel{eq-squared-exponential}{{5}{2}{Gaussian Processes}{equation.2.5}{}}
\citation{jorge1980}
\citation{foster2009}
\citation{snelson2005}
\newlabel{eq-log-marginal-likelihood}{{6}{3}{Gaussian Processes}{equation.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Sparse Gaussian Processes}{3}{section.3}}
\newlabel{sec-sparse-gaussian-processes}{{3}{3}{Sparse Gaussian Processes}{section.3}{}}
\newlabel{eq-linear-regression-objective}{{7}{3}{Sparse Gaussian Processes}{equation.3.7}{}}
\newlabel{eq-linear-regression-objective-rectangular}{{8}{3}{Sparse Gaussian Processes}{equation.3.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Proposed Approach}{3}{section.4}}
\newlabel{sec-proposed-approach}{{4}{3}{Proposed Approach}{section.4}{}}
\newlabel{eq-squared-exponential-extension}{{9}{3}{Proposed Approach}{equation.4.9}{}}
\newlabel{eq-dfdl}{{10a}{3}{Proposed Approach}{equation.4.10a}{}}
\newlabel{eq-dfdp}{{10b}{3}{Proposed Approach}{equation.4.10b}{}}
\citation{rasmussen2006gaussian}
\citation{roberts2012rs}
\citation{weiss2007}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Synthetic regression problem generated from a mixture of random Gaussian kernels. \relax }}{4}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig-toy-example}{{1}{4}{Synthetic regression problem generated from a mixture of random Gaussian kernels. \relax }{figure.caption.1}{}}
\newlabel{eq-squared-exponential-covariance-extension}{{11}{4}{Proposed Approach}{equation.4.11}{}}
\newlabel{eq-Cinv}{{12a}{4}{Proposed Approach}{equation.4.12a}{}}
\newlabel{eq-V_j}{{12b}{4}{Proposed Approach}{equation.4.12b}{}}
\newlabel{eq-squared-exponential-covariance-extension-simplified}{{13}{4}{Proposed Approach}{equation.4.13}{}}
\newlabel{eq-dfdL}{{14a}{4}{Proposed Approach}{equation.4.14a}{}}
\newlabel{eq-dfdP}{{14b}{4}{Proposed Approach}{equation.4.14b}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Linear Regression Prior}{4}{subsection.4.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparisons between different sparse GP approaches with 1 to 4 basis functions (top to bottom) using (a) a global length scale, (b) variable length scales and (c) variable covariances.\relax }}{4}{figure.caption.2}}
\newlabel{fig-toy-comparison}{{2}{4}{Comparisons between different sparse GP approaches with 1 to 4 basis functions (top to bottom) using (a) a global length scale, (b) variable length scales and (c) variable covariances.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Cost Sensitive Learning}{4}{subsection.4.2}}
\citation{ilbert06}
\citation{collister04}
\citation{hildebrandt10,abdalla11,sanchez14}
\citation{bonfield10}
\citation{jouvel09}
\citation{jolliffe1986}
\citation{lecun1998}
\citation{foster2009}
\newlabel{eq-weighted-linear-regression-objective}{{15}{5}{Cost Sensitive Learning}{equation.4.15}{}}
\newlabel{eq-weighted-linear-regression-objective-rectangular}{{16}{5}{Cost Sensitive Learning}{equation.4.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Application to Photometric Redshift Estimation}{5}{section.5}}
\newlabel{sec-application}{{5}{5}{Application to Photometric Redshift Estimation}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Dataset}{5}{subsection.5.1}}
\newlabel{sec-dataset}{{5.1}{5}{Dataset}{subsection.5.1}{}}
\citation{laureijs2011}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Using coordinate-descent to optimise the linear regression objective on the synthetic dataset shown in Figure \ref  {fig-toy-example}. Subfigure (a) shows the steps taken when the data has some correlation, while subfigure (b) shows the steps taken when the data was de-correlated.\relax }}{6}{figure.caption.3}}
\newlabel{fig-error-surface}{{3}{6}{Using coordinate-descent to optimise the linear regression objective on the synthetic dataset shown in Figure \ref {fig-toy-example}. Subfigure (a) shows the steps taken when the data has some correlation, while subfigure (b) shows the steps taken when the data was de-correlated.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The spectroscopic redshift distribution of the full dataset.\relax }}{6}{figure.caption.4}}
\newlabel{fig-zspec-hostogram}{{4}{6}{The spectroscopic redshift distribution of the full dataset.\relax }{figure.caption.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Experiments and Results}{6}{section.6}}
\newlabel{sec-experiments}{{6}{6}{Experiments and Results}{section.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance measures for each algorithm trained using $m=10$ basis functions.\relax }}{6}{table.caption.6}}
\newlabel{table-experiment-1}{{1}{6}{Performance measures for each algorithm trained using $m=10$ basis functions.\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Modelling Performance}{6}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Prior Mean}{6}{subsection.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Weighted Samples}{6}{subsection.6.3}}
\citation{stableGP}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Density scatter plots of the true $z_{spec}$ vs the predicted $z_{phot}$ for (a) ANN, (b) stableGP, (c) GP-GL, (d) GP-VL and (e) GP-VC using $m=10$ basis functions.\relax }}{7}{figure.caption.5}}
\newlabel{fig-experiment-1}{{5}{7}{Density scatter plots of the true $z_{spec}$ vs the predicted $z_{phot}$ for (a) ANN, (b) stableGP, (c) GP-GL, (d) GP-VL and (e) GP-VC using $m=10$ basis functions.\relax }{figure.caption.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The $\Delta z$ for each algorithm trained using $m=10$ basis functions\relax }}{7}{table.caption.7}}
\newlabel{table-RIZ-splits}{{2}{7}{The $\Delta z$ for each algorithm trained using $m=10$ basis functions\relax }{table.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Size of the basis set}{7}{subsection.6.4}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Performance measures of training the GP-VC model using $m=10$ basis functions and different weighting schemes.\relax }}{7}{table.caption.10}}
\newlabel{table-normal-balanced}{{3}{7}{Performance measures of training the GP-VC model using $m=10$ basis functions and different weighting schemes.\relax }{table.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Density scatter plots of the true $z_{spec}$ vs the predicted $z_{phot}$ after training the GP-VC model with samples with $RIZ<23$ (top) and $RIZ<22$ (bottom) using $m=10$ basis functions with (a) zero mean, (b) linear regression and (c) joint linear and non-linear optimisation\relax }}{8}{figure.caption.8}}
\newlabel{fig-RIZ-splits}{{6}{8}{Density scatter plots of the true $z_{spec}$ vs the predicted $z_{phot}$ after training the GP-VC model with samples with $RIZ<23$ (top) and $RIZ<22$ (bottom) using $m=10$ basis functions with (a) zero mean, (b) linear regression and (c) joint linear and non-linear optimisation\relax }{figure.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The time complexity of each approach.\relax }}{8}{table.caption.11}}
\newlabel{table-time-complexity}{{4}{8}{The time complexity of each approach.\relax }{table.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Size of the training set}{8}{subsection.6.5}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{8}{section.7}}
\newlabel{sec-conclusion}{{7}{8}{Conclusion}{section.7}{}}
\bibstyle{mn2e}
\bibdata{sources}
\bibcite{stableGP}{{1}{2010}{{Ashok Srivastava}}{{}}}
\bibcite{foster2009}{{2}{2009}{{Foster {et~al}\unhbox \voidb@x \hbox {.}}}{{Foster, Waagen, Aijaz, Hurley, Luis, Rinsky, Satyavolu, Way, Gazis, \& Srivastava}}}
\bibcite{gibbs97}{{3}{1997}{{Gibbs \& MacKay}}{{}}}
\bibcite{jolliffe1986}{{4}{1986}{{Jolliffe}}{{}}}
\bibcite{laureijs2011}{{5}{2011}{{{Laureijs} {et~al}\unhbox \voidb@x \hbox {.}}}{{{Laureijs}, {Amiaux}, {Arduini}, {Augu{\`e}res}, {Brinchmann}, {Cole}, {Cropper}, {Dabin}, {Duvet}, {Ealet}, \& et~al.}}}
\bibcite{lecun1998}{{6}{1998}{{LeCun {et~al}\unhbox \voidb@x \hbox {.}}}{{LeCun, Bottou, Orr, \& Mueller}}}
\bibcite{jorge1980}{{7}{1980}{{Nocedal}}{{}}}
\bibcite{rasmussen2006gaussian}{{8}{2006}{{Rasmussen \& Williams}}{{}}}
\bibcite{roberts2012rs}{{9}{2013}{{Roberts {et~al}\unhbox \voidb@x \hbox {.}}}{{Roberts, Osborne, Ebden, Reece, Gibson, \& Aigrain}}}
\newlabel{fig-normal}{{7a}{9}{Normal\relax }{figure.caption.9}{}}
\newlabel{sub@fig-normal}{{a}{9}{Normal\relax }{figure.caption.9}{}}
\newlabel{fig-balanced}{{7b}{9}{Balanced\relax }{figure.caption.9}{}}
\newlabel{sub@fig-balanced}{{b}{9}{Balanced\relax }{figure.caption.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Box plots of residual errors on the hold-out test set, showing median (bar), inter-quartile range (box) and range (whiskers) for (a) the direct sum of squared errors and (b) the balanced cost sensitive learning for the GV-VC model. All training was with $m=10$ basis functions for comparison. The right-most histograms are the empirical densities of errors.\relax }}{9}{figure.caption.9}}
\newlabel{fig-normal-balanced}{{7}{9}{Box plots of residual errors on the hold-out test set, showing median (bar), inter-quartile range (box) and range (whiskers) for (a) the direct sum of squared errors and (b) the balanced cost sensitive learning for the GV-VC model. All training was with $m=10$ basis functions for comparison. The right-most histograms are the empirical densities of errors.\relax }{figure.caption.9}{}}
\newlabel{fig-rmses}{{8a}{9}{Root Mean Squared Error.\relax }{figure.caption.12}{}}
\newlabel{sub@fig-rmses}{{a}{9}{Root Mean Squared Error.\relax }{figure.caption.12}{}}
\newlabel{fig-time-seconds}{{8b}{9}{Time.\relax }{figure.caption.12}{}}
\newlabel{sub@fig-time-seconds}{{b}{9}{Time.\relax }{figure.caption.12}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The root mean squares (a) and the time in seconds per iteration (b) as a function of $m$ for all the models.\relax }}{9}{figure.caption.12}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The $\Delta z$ and $\Delta z_{norm}$ after training GP-VC models with $m=200$ basis functions with various training set percentage splits. \relax }}{9}{figure.caption.13}}
\newlabel{fig-training-percentage}{{9}{9}{The $\Delta z$ and $\Delta z_{norm}$ after training GP-VC models with $m=200$ basis functions with various training set percentage splits. \relax }{figure.caption.13}{}}
\bibcite{snelson2005}{{10}{2006}{{Snelson \& Ghahramani}}{{}}}
\bibcite{tsiligkaridis2013}{{11}{2013}{{Tsiligkaridis \& Hero}}{{}}}
\bibcite{weiss2007}{{12}{2007}{{Weiss, McCarthy \& Zabar}}{{Weiss, McCarthy, \& Zabar}}}
\bibcite{zhang2005time}{{13}{2005}{{Zhang, Leithead \& Leith}}{{Zhang, Leithead, \& Leith}}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The density scatter plot for the final GP-VC model trained using $m=200$ basis functions with a jointly optimised linear mean function, a balanced and normalised weights. \relax }}{10}{figure.caption.14}}
\newlabel{fig-final-model}{{10}{10}{The density scatter plot for the final GP-VC model trained using $m=200$ basis functions with a jointly optimised linear mean function, a balanced and normalised weights. \relax }{figure.caption.14}{}}
\newlabel{lastpage}{{7}{10}{Conclusion}{section*.15}{}}
