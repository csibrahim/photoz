\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{Lilly2009,LeFevre2013,LeFevre2015}
\citation{2dfgrs,GAMA,SDSS3}
\citation{Hyperz}
\citation{ZEBRA}
\citation{EAZY}
\citation{Ilbert2006}
\citation{Firth2003,Collister04}
\citation{Ball2008}
\citation{Hogan2015}
\citation{Geach2012}
\newlabel{firstpage}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{hildebrandt10,abdalla11,sanchez14}
\citation{Way2009}
\citation{bonfield10}
\citation{rasmussen2006gaussian}
\citation{zhang2005time}
\citation{tsiligkaridis2013}
\citation{gibbs97}
\citation{foster2009}
\citation{snelson2005}
\citation{laureijs2011}
\citation{rasmussen2006gaussian}
\@writefile{toc}{\contentsline {section}{\numberline {2}Gaussian Processes}{2}{section.2}}
\newlabel{sec-gaussian-process}{{2}{2}{Gaussian Processes}{section.2}{}}
\citation{rasmussen2006gaussian}
\citation{rasmussen2006gaussian}
\citation{roberts2012rs}
\citation{jorge1980}
\citation{foster2009}
\citation{snelson2005}
\newlabel{eq-conditional-zero-mean}{{3}{3}{Gaussian Processes}{equation.2.3}{}}
\newlabel{eq-mean-variance-noise}{{4}{3}{Gaussian Processes}{equation.2.4}{}}
\newlabel{eq-squared-exponential}{{5}{3}{Gaussian Processes}{equation.2.5}{}}
\newlabel{eq-log-marginal-likelihood}{{6}{3}{Gaussian Processes}{equation.2.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Sparse Gaussian Processes}{3}{section.3}}
\newlabel{sec-sparse-gaussian-processes}{{3}{3}{Sparse Gaussian Processes}{section.3}{}}
\newlabel{eq-linear-regression-objective}{{7}{3}{Sparse Gaussian Processes}{equation.3.7}{}}
\newlabel{eq-linear-regression-objective-rectangular}{{8}{3}{Sparse Gaussian Processes}{equation.3.8}{}}
\citation{rasmussen2006gaussian}
\citation{roberts2012rs}
\@writefile{toc}{\contentsline {section}{\numberline {4}Proposed Approach}{4}{section.4}}
\newlabel{sec-proposed-approach}{{4}{4}{Proposed Approach}{section.4}{}}
\newlabel{eq-squared-exponential-extension}{{9}{4}{Proposed Approach}{equation.4.9}{}}
\newlabel{eq-dfdl}{{10a}{4}{Proposed Approach}{equation.4.10a}{}}
\newlabel{eq-dfdp}{{10b}{4}{Proposed Approach}{equation.4.10b}{}}
\newlabel{eq-squared-exponential-covariance-extension}{{11}{4}{Proposed Approach}{equation.4.11}{}}
\newlabel{eq-Cinv}{{12a}{4}{Proposed Approach}{equation.4.12a}{}}
\newlabel{eq-V_j}{{12b}{4}{Proposed Approach}{equation.4.12b}{}}
\newlabel{eq-squared-exponential-covariance-extension-simplified}{{13}{4}{Proposed Approach}{equation.4.13}{}}
\newlabel{eq-dfdL}{{14a}{4}{Proposed Approach}{equation.4.14a}{}}
\newlabel{eq-dfdP}{{14b}{4}{Proposed Approach}{equation.4.14b}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Linear Regression Prior}{4}{subsection.4.1}}
\citation{weiss2007}
\citation{jouvel09}
\citation{jolliffe1986}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Synthetic regression problem generated from a mixture of random Gaussian kernels. \relax }}{5}{figure.caption.1}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig-toy-example}{{1}{5}{Synthetic regression problem generated from a mixture of random Gaussian kernels. \relax }{figure.caption.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparisons between different sparse GP approaches with 1 to 4 basis functions (top to bottom) using (a) a global length scale, (b) variable length scales and (c) variable covariances.\relax }}{5}{figure.caption.2}}
\newlabel{fig-toy-comparison}{{2}{5}{Comparisons between different sparse GP approaches with 1 to 4 basis functions (top to bottom) using (a) a global length scale, (b) variable length scales and (c) variable covariances.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Cost Sensitive Learning}{5}{subsection.4.2}}
\newlabel{eq-weighted-linear-regression-objective}{{15}{5}{Cost Sensitive Learning}{equation.4.15}{}}
\newlabel{eq-weighted-linear-regression-objective-rectangular}{{16}{5}{Cost Sensitive Learning}{equation.4.16}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Application to Photometric Redshift Estimation}{5}{section.5}}
\newlabel{sec-application}{{5}{5}{Application to Photometric Redshift Estimation}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Dataset}{5}{subsection.5.1}}
\newlabel{sec-dataset}{{5.1}{5}{Dataset}{subsection.5.1}{}}
\citation{lecun1998}
\citation{foster2009}
\citation{laureijs2011}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Using coordinate-descent to optimise the linear regression objective on the synthetic dataset shown in Figure \ref  {fig-toy-example}. Subfigure (a) shows the steps taken when the data has some correlation, while subfigure (b) shows the steps taken when the data was de-correlated.\relax }}{6}{figure.caption.3}}
\newlabel{fig-error-surface}{{3}{6}{Using coordinate-descent to optimise the linear regression objective on the synthetic dataset shown in Figure \ref {fig-toy-example}. Subfigure (a) shows the steps taken when the data has some correlation, while subfigure (b) shows the steps taken when the data was de-correlated.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces The spectroscopic redshift distribution of the full dataset.\relax }}{6}{figure.caption.4}}
\newlabel{fig-zspec-hostogram}{{4}{6}{The spectroscopic redshift distribution of the full dataset.\relax }{figure.caption.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Performance measures for each algorithm trained using $m=10$ basis functions.\relax }}{6}{table.caption.6}}
\newlabel{table-experiment-1}{{1}{6}{Performance measures for each algorithm trained using $m=10$ basis functions.\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Experiments and Results}{6}{section.6}}
\newlabel{sec-experiments}{{6}{6}{Experiments and Results}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Modelling Performance}{6}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Size of the training set}{6}{subsection.6.2}}
\newlabel{sec-sizetraining}{{6.2}{6}{Size of the training set}{subsection.6.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Density scatter plots of the true $z_{spec}$ vs the predicted $z_{phot}$ for (a) ANN, (b) stableGP, (c) GP-GL, (d) GP-VL and (e) GP-VC using $m=10$ basis functions.\relax }}{7}{figure.caption.5}}
\newlabel{fig-experiment-1}{{5}{7}{Density scatter plots of the true $z_{spec}$ vs the predicted $z_{phot}$ for (a) ANN, (b) stableGP, (c) GP-GL, (d) GP-VL and (e) GP-VC using $m=10$ basis functions.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces The $\Delta z$ and $\Delta z_{norm}$ after training GP-VC models with $m=200$ basis functions with various training set percentage splits. \relax }}{7}{figure.caption.7}}
\newlabel{fig-training-percentage}{{6}{7}{The $\Delta z$ and $\Delta z_{norm}$ after training GP-VC models with $m=200$ basis functions with various training set percentage splits. \relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Prior Mean}{7}{subsection.6.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}Weighted Samples}{7}{subsection.6.4}}
\citation{stableGP}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The $\Delta z$ for each algorithm trained using $m=10$ basis functions\relax }}{8}{table.caption.8}}
\newlabel{table-RIZ-splits}{{2}{8}{The $\Delta z$ for each algorithm trained using $m=10$ basis functions\relax }{table.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Density scatter plots of the true $z_{spec}$ vs the predicted $z_{phot}$ after training the GP-VC model with samples with $RIZ<23$ (top) and $RIZ<22$ (bottom) using $m=10$ basis functions with (a) zero mean, (b) linear regression and (c) joint linear and non-linear optimisation\relax }}{8}{figure.caption.9}}
\newlabel{fig-RIZ-splits}{{7}{8}{Density scatter plots of the true $z_{spec}$ vs the predicted $z_{phot}$ after training the GP-VC model with samples with $RIZ<23$ (top) and $RIZ<22$ (bottom) using $m=10$ basis functions with (a) zero mean, (b) linear regression and (c) joint linear and non-linear optimisation\relax }{figure.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Performance measures of training the GP-VC model using $m=10$ basis functions and different weighting schemes.\relax }}{8}{table.caption.11}}
\newlabel{table-normal-balanced}{{3}{8}{Performance measures of training the GP-VC model using $m=10$ basis functions and different weighting schemes.\relax }{table.caption.11}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.5}Size of the basis set}{8}{subsection.6.5}}
\newlabel{fig-normal}{{8a}{9}{Normal\relax }{figure.caption.10}{}}
\newlabel{sub@fig-normal}{{a}{9}{Normal\relax }{figure.caption.10}{}}
\newlabel{fig-balanced}{{8b}{9}{Balanced\relax }{figure.caption.10}{}}
\newlabel{sub@fig-balanced}{{b}{9}{Balanced\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Box plots of residual errors on the hold-out test set, showing median (bar), inter-quartile range (box) and range (whiskers) for (a) the direct sum of squared errors and (b) the balanced cost sensitive learning for the GV-VC model. All training was with $m=10$ basis functions for comparison. The right-most histograms are the empirical densities of errors.\relax }}{9}{figure.caption.10}}
\newlabel{fig-normal-balanced}{{8}{9}{Box plots of residual errors on the hold-out test set, showing median (bar), inter-quartile range (box) and range (whiskers) for (a) the direct sum of squared errors and (b) the balanced cost sensitive learning for the GV-VC model. All training was with $m=10$ basis functions for comparison. The right-most histograms are the empirical densities of errors.\relax }{figure.caption.10}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces The time complexity of each approach.\relax }}{9}{table.caption.12}}
\newlabel{table-time-complexity}{{4}{9}{The time complexity of each approach.\relax }{table.caption.12}{}}
\newlabel{fig-rmses}{{9a}{9}{Root Mean Squared Error.\relax }{figure.caption.13}{}}
\newlabel{sub@fig-rmses}{{a}{9}{Root Mean Squared Error.\relax }{figure.caption.13}{}}
\newlabel{fig-time-seconds}{{9b}{9}{Time.\relax }{figure.caption.13}{}}
\newlabel{sub@fig-time-seconds}{{b}{9}{Time.\relax }{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The root mean squares (a) and the time in seconds per iteration (b) as a function of $m$ for all the models.\relax }}{9}{figure.caption.13}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{9}{section.7}}
\newlabel{sec-conclusion}{{7}{9}{Conclusion}{section.7}{}}
\citation{jouvel09}
\bibstyle{mn2e}
\bibdata{sources}
\bibcite{abdalla11}{{1}{2011}{{{Abdalla} {et~al}\unhbox \voidb@x \hbox {.}}}{{{Abdalla}, {Banerji}, {Lahav}, \& {Rashkov}}}}
\bibcite{SDSS3}{{2}{2015}{{{Alam} {et~al}\unhbox \voidb@x \hbox {.}}}{{{Alam}, {Albareti}, {Allende Prieto}, {Anders}, {Anderson}, {Andrews}, {Armengaud}, {Aubourg}, {Bailey}, {Bautista}, \& et~al.}}}
\bibcite{stableGP}{{3}{2010}{{Ashok Srivastava}}{{}}}
\bibcite{Ball2008}{{4}{2008}{{{Ball} {et~al}\unhbox \voidb@x \hbox {.}}}{{{Ball}, {Brunner}, {Myers}, {Strand}, {Alberts}, \& {Tcheng}}}}
\bibcite{Hyperz}{{5}{2000}{{{Bolzonella}, {Miralles} \& {Pell{\'o}}}}{{{Bolzonella}, {Miralles}, \& {Pell{\'o}}}}}
\bibcite{bonfield10}{{6}{2010}{{{Bonfield} {et~al}\unhbox \voidb@x \hbox {.}}}{{{Bonfield}, {Sun}, {Davey}, {Jarvis}, {Abdalla}, {Banerji}, \& {Adams}}}}
\bibcite{EAZY}{{7}{2008}{{{Brammer}, {van Dokkum} \& {Coppi}}}{{{Brammer}, {van Dokkum}, \& {Coppi}}}}
\bibcite{2dfgrs}{{8}{2003}{{{Colless} {et~al}\unhbox \voidb@x \hbox {.}}}{{{Colless}, {Peterson}, {Jackson}, {Peacock}, {Cole}, {Norberg}, {Baldry}, {Baugh}, {Bland-Hawthorn}, {Bridges}, {Cannon}, {Collins}, {Couch}, {Cross}, {Dalton}, {De Propris}, {Driver}, {Efstathiou}, {Ellis}, {Frenk}, {Glazebrook}, {Lahav}, {Lewis}, {Lumsden}, {Maddox}, {Madgwick}, {Sutherland}, \& {Taylor}}}}
\bibcite{Collister04}{{9}{2004}{{{Collister} \& {Lahav}}}{{}}}
\bibcite{GAMA}{{10}{2011}{{{Driver} {et~al}\unhbox \voidb@x \hbox {.}}}{{{Driver}, {Hill}, {Kelvin}, {Robotham}, {Liske}, {Norberg}, {Baldry}, {Bamford}, {Hopkins}, {Loveday}, {Peacock}, {Andrae}, {Bland-Hawthorn}, {Brough}, {Brown}, {Cameron}, {Ching}, {Colless}, {Conselice}, {Croom}, {Cross}, {de Propris}, {Dye}, {Drinkwater}, {Ellis}, {Graham}, {Grootes}, {Gunawardhana}, {Jones}, {van Kampen}, {Maraston}, {Nichol}, {Parkinson}, {Phillipps}, {Pimbblet}, {Popescu}, {Prescott}, {Roseboom}, {Sadler}, {Sansom}, {Sharp}, {Smith}, {Taylor}, {Thomas}, {Tuffs}, {Wijesinghe}, {Dunne}, {Frenk}, {Jarvis}, {Madore}, {Meyer}, {Seibert}, {Staveley-Smith}, {Sutherland}, \& {Warren}}}}
\bibcite{ZEBRA}{{11}{2006}{{{Feldmann} {et~al}\unhbox \voidb@x \hbox {.}}}{{{Feldmann}, {Carollo}, {Porciani}, {Lilly}, {Capak}, {Taniguchi}, {Le F{\`e}vre}, {Renzini}, {Scoville}, {Ajiki}, {Aussel}, {Contini}, {McCracken}, {Mobasher}, {Murayama}, {Sanders}, {Sasaki}, {Scarlata}, {Scodeggio}, {Shioya}, {Silverman}, {Takahashi}, {Thompson}, \& {Zamorani}}}}
\bibcite{Firth2003}{{12}{2003}{{{Firth}, {Lahav} \& {Somerville}}}{{{Firth}, {Lahav}, \& {Somerville}}}}
\bibcite{foster2009}{{13}{2009}{{Foster {et~al}\unhbox \voidb@x \hbox {.}}}{{Foster, Waagen, Aijaz, Hurley, Luis, Rinsky, Satyavolu, Way, Gazis, \& Srivastava}}}
\bibcite{Geach2012}{{14}{2012}{{Geach}}{{}}}
\bibcite{gibbs97}{{15}{1997}{{Gibbs \& MacKay}}{{}}}
\bibcite{hildebrandt10}{{16}{2010}{{{Hildebrandt} {et~al}\unhbox \voidb@x \hbox {.}}}{{{Hildebrandt}, {Arnouts}, {Capak}, {Moustakas}, {Wolf}, {Abdalla}, {Assef}, {Banerji}, {Ben{\'{\i }}tez}, {Brammer}, {Budav{\'a}ri}, {Carliles}, {Coe}, {Dahlen}, {Feldmann}, {Gerdes}, {Gillis}, {Ilbert}, {Kotulla}, {Lahav}, {Li}, {Miralles}, {Purger}, {Schmidt}, \& {Singal}}}}
\bibcite{Hogan2015}{{17}{2015}{{{Hogan}, {Fairbairn} \& {Seeburn}}}{{{Hogan}, {Fairbairn}, \& {Seeburn}}}}
\bibcite{Ilbert2006}{{18}{2006}{{{Ilbert} {et~al}\unhbox \voidb@x \hbox {.}}}{{{Ilbert}, {Arnouts}, {McCracken}, {Bolzonella}, {Bertin}, {Le F{\`e}vre}, {Mellier}, {Zamorani}, {Pell{\`o}}, {Iovino}, {Tresse}, {Le Brun}, {Bottini}, {Garilli}, {Maccagni}, {Picat}, {Scaramella}, {Scodeggio}, {Vettolani}, {Zanichelli}, {Adami}, {Bardelli}, {Cappi}, {Charlot}, {Ciliegi}, {Contini}, {Cucciati}, {Foucaud}, {Franzetti}, {Gavignaud}, {Guzzo}, {Marano}, {Marinoni}, {Mazure}, {Meneux}, {Merighi}, {Paltani}, {Pollo}, {Pozzetti}, {Radovich}, {Zucca}, {Bondi}, {Bongiorno}, {Busarello}, {de La Torre}, {Gregorini}, {Lamareille}, {Mathez}, {Merluzzi}, {Ripepi}, {Rizzo}, \& {Vergani}}}}
\bibcite{jolliffe1986}{{19}{1986}{{Jolliffe}}{{}}}
\bibcite{jouvel09}{{20}{2009}{{{Jouvel} {et~al}\unhbox \voidb@x \hbox {.}}}{{{Jouvel}, {Kneib}, {Ilbert}, {Bernstein}, {Arnouts}, {Dahlen}, {Ealet}, {Milliard}, {Aussel}, {Capak}, {Koekemoer}, {Le Brun}, {McCracken}, {Salvato}, \& {Scoville}}}}
\bibcite{laureijs2011}{{21}{2011}{{{Laureijs} {et~al}\unhbox \voidb@x \hbox {.}}}{{{Laureijs}, {Amiaux}, {Arduini}, {Augu{\`e}res}, {Brinchmann}, {Cole}, {Cropper}, {Dabin}, {Duvet}, {Ealet}, \& et~al.}}}
\bibcite{LeFevre2013}{{22}{2013}{{{Le F{\`e}vre} {et~al}\unhbox \voidb@x \hbox {.}}}{{{Le F{\`e}vre}, {Cassata}, {Cucciati}, {Garilli}, {Ilbert}, {Le Brun}, {Maccagni}, {Moreau}, {Scodeggio}, {Tresse}, {Zamorani}, {Adami}, {Arnouts}, {Bardelli}, {Bolzonella}, {Bondi}, {Bongiorno}, {Bottini}, {Cappi}, {Charlot}, {Ciliegi}, {Contini}, {de la Torre}, {Foucaud}, {Franzetti}, {Gavignaud}, {Guzzo}, {Iovino}, {Lemaux}, {L{\'o}pez-Sanjuan}, {McCracken}, {Marano}, {Marinoni}, {Mazure}, {Mellier}, {Merighi}, {Merluzzi}, {Paltani}, {Pell{\`o}}, {Pollo}, {Pozzetti}, {Scaramella}, {Tasca}, {Vergani}, {Vettolani}, {Zanichelli}, \& {Zucca}}}}
\bibcite{LeFevre2015}{{23}{2015}{{{Le F{\`e}vre} {et~al}\unhbox \voidb@x \hbox {.}}}{{{Le F{\`e}vre}, {Tasca}, {Cassata}, {Garilli}, {Le Brun}, {Maccagni}, {Pentericci}, {Thomas}, {Vanzella}, {Zamorani}, {Zucca}, {Amorin}, {Bardelli}, {Capak}, {Cassar{\`a}}, {Castellano}, {Cimatti}, {Cuby}, {Cucciati}, {de la Torre}, {Durkalec}, {Fontana}, {Giavalisco}, {Grazian}, {Hathi}, {Ilbert}, {Lemaux}, {Moreau}, {Paltani}, {Ribeiro}, {Salvato}, {Schaerer}, {Scodeggio}, {Sommariva}, {Talia}, {Taniguchi}, {Tresse}, {Vergani}, {Wang}, {Charlot}, {Contini}, {Fotopoulou}, {L{\'o}pez-Sanjuan}, {Mellier}, \& {Scoville}}}}
\bibcite{lecun1998}{{24}{1998}{{LeCun {et~al}\unhbox \voidb@x \hbox {.}}}{{LeCun, Bottou, Orr, \& Mueller}}}
\bibcite{Lilly2009}{{25}{2009}{{{Lilly} {et~al}\unhbox \voidb@x \hbox {.}}}{{{Lilly}, {Le Brun}, {Maier}, {Mainieri}, {Mignoli}, {Scodeggio}, {Zamorani}, {Carollo}, {Contini}, {Kneib}, {Le F{\`e}vre}, {Renzini}, {Bardelli}, {Bolzonella}, {Bongiorno}, {Caputi}, {Coppa}, {Cucciati}, {de la Torre}, {de Ravel}, {Franzetti}, {Garilli}, {Iovino}, {Kampczyk}, {Kovac}, {Knobel}, {Lamareille}, {Le Borgne}, {Pello}, {Peng}, {P{\'e}rez-Montero}, {Ricciardelli}, {Silverman}, {Tanaka}, {Tasca}, {Tresse}, {Vergani}, {Zucca}, {Ilbert}, {Salvato}, {Oesch}, {Abbas}, {Bottini}, {Capak}, {Cappi}, {Cassata}, {Cimatti}, {Elvis}, {Fumana}, {Guzzo}, {Hasinger}, {Koekemoer}, {Leauthaud}, {Maccagni}, {Marinoni}, {McCracken}, {Memeo}, {Meneux}, {Porciani}, {Pozzetti}, {Sanders}, {Scaramella}, {Scarlata}, {Scoville}, {Shopbell}, \& {Taniguchi}}}}
\bibcite{jorge1980}{{26}{1980}{{Nocedal}}{{}}}
\bibcite{rasmussen2006gaussian}{{27}{2006}{{Rasmussen \& Williams}}{{}}}
\bibcite{roberts2012rs}{{28}{2013}{{Roberts {et~al}\unhbox \voidb@x \hbox {.}}}{{Roberts, Osborne, Ebden, Reece, Gibson, \& Aigrain}}}
\bibcite{sanchez14}{{29}{2014}{{{S{\'a}nchez} {et~al}\unhbox \voidb@x \hbox {.}}}{{{S{\'a}nchez}, {Carrasco Kind}, {Lin}, {Miquel}, {Abdalla}, {Amara}, {Banerji}, {Bonnett}, {Brunner}, {Capozzi}, {Carnero}, {Castander}, {da Costa}, {Cunha}, {Fausti}, {Gerdes}, {Greisel}, {Gschwend}, {Hartley}, {Jouvel}, {Lahav}, {Lima}, {Maia}, {Mart{\'{\i }}}, {Ogando}, {Ostrovski}, {Pellegrini}, {Rau}, {Sadeh}, {Seitz}, {Sevilla-Noarbe}, {Sypniewski}, {de Vicente}, {Abbot}, {Allam}, {Atlee}, {Bernstein}, {Bernstein}, {Buckley-Geer}, {Burke}, {Childress}, {Davis}, {DePoy}, {Dey}, {Desai}, {Diehl}, {Doel}, {Estrada}, {Evrard}, {Fern{\'a}ndez}, {Finley}, {Flaugher}, {Frieman}, {Gaztanaga}, {Glazebrook}, {Honscheid}, {Kim}, {Kuehn}, {Kuropatkin}, {Lidman}, {Makler}, {Marshall}, {Nichol}, {Roodman}, {S{\'a}nchez}, {Santiago}, {Sako}, {Scalzo}, {Smith}, {Swanson}, {Tarle}, {Thomas}, {Tucker}, {Uddin}, {Vald{\'e}s}, {Walker}, {Yuan}, \& {Zuntz}}}}
\bibcite{snelson2005}{{30}{2006}{{Snelson \& Ghahramani}}{{}}}
\bibcite{tsiligkaridis2013}{{31}{2013}{{Tsiligkaridis \& Hero}}{{}}}
\bibcite{Way2009}{{32}{2009}{{{Way} {et~al}\unhbox \voidb@x \hbox {.}}}{{{Way}, {Foster}, {Gazis}, \& {Srivastava}}}}
\bibcite{weiss2007}{{33}{2007}{{Weiss, McCarthy \& Zabar}}{{Weiss, McCarthy, \& Zabar}}}
\bibcite{zhang2005time}{{34}{2005}{{Zhang, Leithead \& Leith}}{{Zhang, Leithead, \& Leith}}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The $\Delta z$ and $\Delta z_{norm}$ after training GP-VC models with 5, 10, 25, 50, 100, 200, 400, 800 and 1600 with cost sensitive learning and joint mean optimisation. The results for the $\Delta z$ were optimised using normal sum of squares whereas the results for the $\Delta z_{norm}$ were optimised using normalised weights\relax }}{10}{figure.caption.14}}
\newlabel{fig-1600}{{10}{10}{The $\Delta z$ and $\Delta z_{norm}$ after training GP-VC models with 5, 10, 25, 50, 100, 200, 400, 800 and 1600 with cost sensitive learning and joint mean optimisation. The results for the $\Delta z$ were optimised using normal sum of squares whereas the results for the $\Delta z_{norm}$ were optimised using normalised weights\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The density scatter plot for the final GP-VC model trained using $m=200$ basis functions with a jointly optimised linear mean function, a balanced and normalised weights. \relax }}{10}{figure.caption.15}}
\newlabel{fig-final-model}{{11}{10}{The density scatter plot for the final GP-VC model trained using $m=200$ basis functions with a jointly optimised linear mean function, a balanced and normalised weights. \relax }{figure.caption.15}{}}
\newlabel{lastpage}{{7}{10}{Conclusion}{section*.16}{}}
